<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zhicheng Sun</title>

    <meta name="author" content="Zhicheng Sun">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/icon.png">
    <meta name="google-site-verification" content="3k2iM7c_w5qn3HQtP_iwpSd-zFu-hQRULkyH8jPqLDc" />
</head>

<body>
<table style="width:800px;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0">
        <td style="padding:0">
            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0">
                    <td style="padding:2.5%;width:66%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Zhicheng Sun (孙至诚)</name>
                        </p>
                        <p>I am a final-year Ph.D. candidate focusing on computer vision and machine learning at Peking University,
                            where my advisor is Prof. <a href="http://www.muyadong.com">Yadong Mu</a>.
                            Before that, I obtained my B.S. degree in Computer Science from Peking University.
                        </p>
                        <p>
                            Email: sunzc [at] pku [dot] edu [dot] cn
                        </p>
                        <p style="text-align:center">
                            <a href="data/Zhicheng_Sun.pdf">CV</a>
                            &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?user=Xa8dgkYAAAAJ">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/zhicheng-sun">LinkedIn</a> &nbsp/&nbsp
                            <a href="https://github.com/feifeiobama">Github</a> &nbsp/&nbsp
                            <a href="https://twitter.com/feifeiobama">Twitter</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/ZhichengSun.jpg"><img style="width:100%;max-width:100%" alt="profile photo"
                            src="images/ZhichengSun.jpg" class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:2.5%;width:100%;vertical-align:middle">
                        <heading>Research</heading>
                        <p>
                            My research spans <a href="https://openai.com/index/generative-models/"><b>Generative Models</b></a> and <a href="https://www.cs.uic.edu/~liub/lifelong-machine-learning.html"><b>Continual Learning</b></a>. My short-term goal is to improve generative models from the continual learning perspective (e.g. efficiency, knowledge sharing). My long-term goal is to enable generative models to learn continually (through new paradigms such as flow-based models and equilibrium models).
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                
                <tr>
                    <td style="padding:2.5%;padding-right:20px;padding-bottom:0px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/Pyramid-Flow.jpg' width="180px" style="opacity:0.9" alt="Pyramid-Flow">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;width:75%;vertical-align:top">
                        <a href="https://arxiv.org/abs/2410.05954">
                            <papertitle>Pyramidal Flow Matching for Efficient Video Generative Modeling</papertitle>
                        </a>
                        <br>
                        <a href="https://jy0205.github.io">Yang Jin</a>,
                        <strong>Zhicheng Sun</strong>,
                        <a href="https://scholar.google.com.hk/citations?user=D5Oz9T8AAAAJ">Ningyuan Li</a>,
                        <a href="https://sites.google.com/view/kunxu/home">Kun Xu</a>,
                        Kun Xu,
                        Hao Jiang,
                        Nan Zhuang,
                        Quzhe Huang,
                        Yang Song,
                        <a href="http://www.muyadong.com">Yadong Mu</a>,
                        <a href="https://zhouchenlin.github.io/">Zhouchen Lin</a>
                        <br>
                        <em>Preprint</em>, 2024
                        <br>
                        <a href="https://pyramid-flow.github.io">project page</a>
                        /
                        <a href="https://arxiv.org/pdf/2410.05954.pdf">paper</a>
                        /
                        <a href="https://github.com/jy0205/Pyramid-Flow">code</a>
                        /
                        <a href="data/Pyramid-Flow.bib">bibtex</a>
                        <p></p>
                        <p>
                            We design a unified flow matching objective that jointly generates
                            and decompresses visual content across pyramidal representations.
                        </p>
                    </td>
                </tr>
                
                <tr>
                    <td style="padding:2.5%;padding-right:20px;padding-bottom:0px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/Equilibrium-Planner.jpg' width="180px" alt="Equilibrium-Planner">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;padding-bottom:0px;width:75%;vertical-align:top">
                        <a href="https://arxiv.org/abs/2410.01440">
                            <papertitle>Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling</papertitle>
                        </a>
                        <br>
                        <a href="https://github.com/Singularity0104">Jinghan Li</a>,
                        <strong>Zhicheng Sun</strong>,
                        Fei Li,
                        Cao Sheng,
                        Jiazhong Yu,
                        <a href="http://www.muyadong.com">Yadong Mu</a>
                        <br>
                        <em>Preprint</em>, 2024
                        <br>
                        <a href="https://arxiv.org/pdf/2410.01440.pdf">paper</a>
                        /
                        <a href="https://github.com/Singularity0104/equilibrium-planner">code</a>
                        /
                        <a href="data/Equilibrium-Planner.bib">bibtex</a>
                        <p></p>
                        <p>
                            We propose an equilibrium model-based LLM planner that is capable of
                            self-refining its plan with external and internal feedback.
                        </p>
                    </td>
                </tr>
                
                <tr>
                    <td style="padding:2.5%;padding-right:20px;padding-bottom:0px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/RectifID.jpg' width="180px" alt="RectifID">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;width:75%;vertical-align:top">
                        <a href="https://arxiv.org/abs/2405.14677">
                            <papertitle>RectifID: Personalizing Rectified Flow with Anchored Classifier Guidance</papertitle>
                        </a>
                        <br>
                        <strong>Zhicheng Sun</strong>,
                        <a href="https://jeffreyyzh.github.io">Zhenhao Yang</a>,
                        <a href="https://jy0205.github.io">Yang Jin</a>,
                        <a href="https://scholar.google.com/citations?user=l5NbV2EAAAAJ">Haozhe Chi</a>,
                        <a href="https://sites.google.com/view/kunxu/home">Kun Xu</a>,
                        Kun Xu,
                        Liwei Chen,
                        Hao Jiang,
                        Yang Song,
                        Kun Gai,
                        <a href="http://www.muyadong.com">Yadong Mu</a>
                        <br>
                        <em>NeurIPS</em>, 2024
                        <br>
                        <a href="https://arxiv.org/pdf/2405.14677.pdf">paper</a>
                        /
                        <a href="https://github.com/feifeiobama/RectifID">code</a>
                        /
                        <a href="data/RectifID.bib">bibtex</a>
                        <p></p>
                        <p>
                            We introduce a training-free approach to personalizing rectified flow,
                            based on a fixed-point formulation of classifier guidance.
                        </p>
                    </td>
                </tr>
                
                <tr>
                    <td style="padding:2.5%;padding-right:20px;padding-bottom:0px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/Video-LaVIT.jpg' width="180px" style="opacity:0.9" alt="Video-LaVIT">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;width:75%;vertical-align:top">
                        <a href="https://arxiv.org/abs/2402.03161">
                            <papertitle>Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization</papertitle>
                        </a>
                        <br>
                        <a href="https://jy0205.github.io">Yang Jin</a>,
                        <strong>Zhicheng Sun</strong>,
                        <a href="https://sites.google.com/view/kunxu/home">Kun Xu</a>,
                        Kun Xu,
                        Liwei Chen,
                        Hao Jiang,
                        Quzhe Huang,
                        Chengru Song,
                        Yuliang Liu,
                        Di Zhang,
                        Yang Song,
                        Kun Gai,
                        <a href="http://www.muyadong.com">Yadong Mu</a>
                        <br>
                        <em>ICML</em>, 2024 &nbsp <span style="color: #FF8080"><strong>(Oral)</strong></span>
                        <br>
                        <a href="https://video-lavit.github.io">project page</a>
                        /
                        <a href="https://arxiv.org/pdf/2402.03161.pdf">paper</a>
                        /
                        <a href="https://github.com/jy0205/LaVIT">code</a>
                        /
                        <a href="data/Video-LaVIT.bib">bibtex</a>
                        <p></p>
                        <p>
                            We present a multimodal LLM capable of both comprehending and generating videos,
                            based on an efficient decomposed video representation.
                        </p>
                    </td>
                </tr>
                
                <tr>
                    <td style="padding:2.5%;padding-right:20px;padding-bottom:0px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/OrthogonalDet.png' width="180px" alt="OrthogonalDet">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;padding-bottom:0px;width:75%;vertical-align:top">
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Sun_Exploring_Orthogonality_in_Open_World_Object_Detection_CVPR_2024_paper.html">
                            <papertitle>Exploring Orthogonality in Open World Object Detection</papertitle>
                        </a>
                        <br>
                        <strong>Zhicheng Sun</strong>,
                        <a href="https://github.com/Singularity0104">Jinghan Li</a>,
                        <a href="http://www.muyadong.com">Yadong Mu</a>
                        <br>
                        <em>CVPR</em>, 2024
                        <br>
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Exploring_Orthogonality_in_Open_World_Object_Detection_CVPR_2024_paper.pdf">paper</a>
                        /
                        <a href="https://github.com/feifeiobama/OrthogonalDet">code</a>
                        /
                        <a href="data/OrthogonalDet.bib">bibtex</a>
                        <p></p>
                        <p>
                            We develop an open world object detector that exploits feature and
                            prediction orthogonality to continually identify unknown objects.
                        </p>
                    </td>
                </tr>
                
                <tr>
                    <td style="padding:2.5%;padding-right:20px;padding-bottom:0px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/InMark.jpg' width="180px" style="opacity:0.6" alt="InMark">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;padding-bottom:0px;width:75%;vertical-align:top">
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Countering_Personalized_Text-to-Image_Generation_with_Influence_Watermarks_CVPR_2024_paper.html">
                            <papertitle>Countering Personalized Text-to-Image Generation with Influence Watermarks</papertitle>
                        </a>
                        <br>
                        <a href="https://hanwenliu.com">Hanwen Liu</a>,
                        <strong>Zhicheng Sun</strong>,
                        <a href="http://www.muyadong.com">Yadong Mu</a>
                        <br>
                        <em>CVPR</em>, 2024
                        <br>
                        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_Countering_Personalized_Text-to-Image_Generation_with_Influence_Watermarks_CVPR_2024_paper.pdf">paper</a>
                        /
                        <a href=".">code</a>
                        /
                        <a href="data/InMark.bib">bibtex</a>
                        <p></p>
                        <p>
                            We forge unlearnable examples against diffusion models with a robust
                            watermarking method that focuses on the most influential pixels.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:2.5%;padding-right:20px;padding-bottom:0px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/RewireNeuron.png' width="180px" alt="RewireNeuron">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;padding-bottom:0px;width:75%;vertical-align:top">
                        <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/599221d7ebf6b3403190f38a3f282a1c-Abstract-Conference.html">
                            <papertitle>Rewiring Neurons in Non-Stationary Environments</papertitle>
                        </a>
                        <br>
                        <strong>Zhicheng Sun</strong>,
                        <a href="http://www.muyadong.com">Yadong Mu</a>
                        <br>
                        <em>NeurIPS</em>, 2023 &nbsp <span style="color: #FF8080"><strong>(Spotlight)</strong></span>
                        <br>
                        <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/599221d7ebf6b3403190f38a3f282a1c-Paper-Conference.pdf">paper</a>
                        /
                        <a href="https://github.com/feifeiobama/RewireNeuron">code</a>
                        /
                        <a href="data/RewireNeuron.bib">bibtex</a>
                        <p></p>
                        <p>
                            We propose a novel rewiring approach by permuting hidden neurons,
                            allowing for structural plasticity in continual reinforcement learning.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:2.5%;padding-right:20px;padding-bottom:0px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/InfluenceCL.png' width="180px" alt="InfluenceCL">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;padding-bottom:0px;width:75%;vertical-align:top">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Regularizing_Second-Order_Influences_for_Continual_Learning_CVPR_2023_paper.html">
                            <papertitle>Regularizing Second-Order Influences for Continual Learning</papertitle>
                        </a>
                        <br>
                        <strong>Zhicheng Sun</strong>,
                        <a href="http://www.muyadong.com">Yadong Mu</a>,
                        <a href="https://www.ganghua.org">Gang Hua</a>
                        <br>
                        <em>CVPR</em>, 2023
                        <br>
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Sun_Regularizing_Second-Order_Influences_for_Continual_Learning_CVPR_2023_paper.pdf">paper</a>
                        /
                        <a href="https://arxiv.org/abs/2304.10177">arXiv</a>
                        /
                        <a href="https://github.com/feifeiobama/InfluenceCL">code</a>
                        /
                        <a href="data/InfluenceCL.bib">bibtex</a>
                        <p></p>
                        <p>
                            We identify a new class of second-order influence functions in replay-based
                            continual learning, and address it with a regularized selection strategy.
                        </p>
                    </td>
                </tr>

                <tr>
                    <td style="padding:2.5%;padding-right:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src='images/PatchKD.jpg' width="180px" style="opacity:0.8" alt="PatchKD">
                        </div>
                    </td>
                    <td style="padding:2.5%;padding-left:20px;padding-right:0px;padding-bottom:0px;width:75%;vertical-align:top">
                        <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548179">
                            <papertitle>Patch-based Knowledge Distillation for Lifelong Person Re-Identification</papertitle>
                        </a>
                        <br>
                        <strong>Zhicheng Sun</strong>,
                        <a href="http://www.muyadong.com">Yadong Mu</a>
                        <br>
                        <em>ACM Multimedia</em>, 2022
                        <br>
                        <a href="http://www.muyadong.com/paper/acmmm22_sunzc.pdf">paper</a>
                        /
                        <a href="https://github.com/feifeiobama/PatchKD">code</a>
                        /
                        <a href="data/PatchKD.bib">bibtex</a>
                        <p></p>
                        <p>
                            We use adaptively-chosen patches (rather than whole images) to pilot the
                            forgetting-resistant distillation for lifelong person re-identification.
                        </p>
                    </td>
                </tr>

                </tbody>
            </table>


            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:2.5%;width:100%;vertical-align:middle">
                        <heading>Service and Teaching</heading>
                        <ul>
                            <li>Conference Reviewer: ICLR 2025, NeurIPS 2024, ICCV 2023, ECCV 2024, ACM Multimedia 2022--2024, AISTATS 2025, ACCV 2024, ACML 2024, WACV 2024</li>
                        </ul>
                        <ul>
                            <li>Journal Reviewer: TMM 2022-2023, TCSVT 2024, Neurocomputing 2022</li>
                        </ul>
                        <ul>
                            <li>Teaching Assistant: Computer Vision and Deep Learning by Prof. <a href="http://www.muyadong.com">Yadong Mu</a>, Spring 2022</li>
                        </ul>
                        <ul>
                            <li>Teaching Assistant: Deep Learning: Algorithm and Application by Prof. <a href="http://www.muyadong.com">Yadong Mu</a>, Spring 2024</li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:2.5%;width:100%;vertical-align:middle">
                        <heading>Miscellaneous</heading>
                        <ul>
                            <li>
                                After the breakthrough of <a href="https://www.nature.com/articles/nature16961">AlphaGo</a>, I became obsessed with AI bots
                                and held a top-5 user ranking on the AI competitive platform <a href="https://en.botzone.org.cn">Botzone</a> for a long time.
                            </li>
                        </ul>
                        <ul>
                            <li>
                                In the <a href="https://github.com/PKU-GeekGame/geekgame-1st">1st PKU GeekGame</a>,
                                I almost succeeded in brute forcing a CTF problem <a href="https://github.com/PKU-GeekGame/geekgame-1st/tree/master/writeups/players/feifeiobama">using an AI algorithm</a>.
                            </li>
                        </ul>
                    </td>
                </tr>
                </tbody>
            </table>


            <table style="width:100%;border:0;border-spacing:0;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0">
                        <p style="text-align:right;font-size:small;">
                            <a href="https://jonbarron.info">Template</a>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>

</html>
